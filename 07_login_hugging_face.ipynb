{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7619f1e3-edac-41a2-a3d8-135f955c65e5",
   "metadata": {},
   "source": [
    "%%capture\n",
    "!git clone 'https://github.com/ali7919/Enlighten-Instruct.git'\n",
    "!pip install -U bitsandbytes\n",
    "!pip install transformers==4.36.2\n",
    "!pip install -U peft\n",
    "!pip install -U accelerate\n",
    "!pip install -U trl\n",
    "!pip install datasets==2.16.0\n",
    "!pip install sentencepiece\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25dc15d",
   "metadata": {},
   "source": [
    "# streamlit run app2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e47de",
   "metadata": {},
   "source": [
    "@st.cache(allow_output_mutation=True, hash_funcs={AutoTokenizer: hash_tokenizer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be49b1f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c983926-024e-4d90-85e3-d58afa86d2b0",
   "metadata": {},
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1782e6f1-4dea-41f6-843b-c2546098bc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b23c21-3625-4655-9805-e15605f0cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "import streamlit as st\n",
    "#from streamlit_chat import message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e216c21-1543-4436-ab16-1a011bbe84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99d0eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv(\"hugging_face.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19ff3fb7-70f9-44ea-a4cf-076f803eb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_token = os.environ.get(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00d7345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bdab84e-4be0-43e4-b5f9-a5719986995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/serlink/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = f\"huggingface-cli login --token {huggingface_token}\"\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6cf394",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "new_model = \"serlikopar/Enlighten_Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "158ca9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c16367ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.04s/it]\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model_reload, new_model)\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b297e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"test_data_62_questions_with_formatting.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d85bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "#pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89da0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "  prompt=f\"<s>[INST] {question} [/INST]\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b7d86dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Was ist avogadro Konstante? [/INST]Die Avogadro-Konstante ist die Zahl der Teilchen in einem Mol. Sie ist 6,022 × 10^23 Teilchen pro Mol.\n"
     ]
    }
   ],
   "source": [
    "question = \"Was ist avogadro Konstante?\"\n",
    "prompt = build_prompt(question)\n",
    "result = pipe(prompt)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0095a6a",
   "metadata": {},
   "source": [
    "# Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c7ec486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline():\n",
    "    return pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "\n",
    "pipe = get_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95ea5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title('Text Generation Model')\n",
    "question = st.text_input(\"Enter your question:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68f52168",
   "metadata": {},
   "outputs": [],
   "source": [
    "if st.button('Generate Answer'):\n",
    "    if question:\n",
    "        with st.spinner('Generating...'):\n",
    "            prompt = build_prompt(question)\n",
    "            result = pipe(prompt)\n",
    "            generated_text = result[0]['generated_text']\n",
    "        st.write(generated_text)\n",
    "    else:\n",
    "        st.write(\"Please enter a question to generate an answer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eeeb7f",
   "metadata": {},
   "source": [
    "streamlit run /opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/ipykernel_launcher.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d766d",
   "metadata": {},
   "source": [
    "# Previous Function for evaluating MCQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b8e57",
   "metadata": {},
   "source": [
    "df_test=pd.read_csv(test_path)\n",
    "\n",
    "questionCounter=0\n",
    "correct=0\n",
    "promptEnding = \"[/INST]\"\n",
    "\n",
    "# this must be >= 2\n",
    "fail_limit=3\n",
    "\n",
    "# chain of thought activator, model might run out of output tokens\n",
    "USE_COT=True\n",
    "\n",
    "#this comes before the question\n",
    "testGuide='Beantworten Sie die folgende Frage. Schreiben Sie die Antwort am Ende Ihrer Antwort in einem Satz:  Die Frage ist:   '\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    print(\"#############################\")\n",
    "    questionCounter = questionCounter + 1\n",
    "\n",
    "    #chain of thought activator\n",
    "    if USE_COT:\n",
    "        chainOfThoughtActivator='\\nPlane zuerst alles Schritt für Schritt durch\\n'\n",
    "    else:\n",
    "        chainOfThoughtActivator='\\n'\n",
    "\n",
    "    #build the prompt\n",
    "    # question=testGuide + row['Question'] + '\\na)' + row['a'] + '\\nb)' + row['b'] + '\\nc)' + row['c'] + '\\nd)' + row['d'] + chainOfThoughtActivator\n",
    "    question=testGuide + row['Question'] + chainOfThoughtActivator\n",
    "    print(question)\n",
    "\n",
    "    #true answer\n",
    "    truth=row['Answer']\n",
    "\n",
    "    #use a loop, if llm stopped before reaching the answer. ask again\n",
    "    index=-1\n",
    "    failCounter=0\n",
    "    while(index==-1):\n",
    "\n",
    "        #build the prompt\n",
    "        prompt = build_prompt(question)\n",
    "\n",
    "        #generate answer\n",
    "        result = pipe(prompt)\n",
    "        llmAnswer = result[0]['generated_text']\n",
    "\n",
    "        #remove our prompt from it\n",
    "        index = llmAnswer.find(promptEnding)\n",
    "        llmAnswer = llmAnswer[len(promptEnding)+index:]\n",
    "\n",
    "        print(\"LLM Answer:\")\n",
    "        print(llmAnswer)\n",
    "\n",
    "        #remove spaces\n",
    "        llmAnswer=llmAnswer.replace(' ','')\n",
    "\n",
    "        #find the option in response\n",
    "        index = llmAnswer.find('Answer:')\n",
    "\n",
    "        #edge case - llm stoped at the worst time\n",
    "        if(index+len('Answer:')==len(llmAnswer)):\n",
    "            index=-1\n",
    "\n",
    "        #update question for the next try. remove chain of thought\n",
    "        # question=testGuide + row['Question'] + '\\na)' + row['a'] + '\\nb)' + row['b'] + '\\nc)' + row['c'] + '\\nd)' + row['d']\n",
    "        question=testGuide + row['Question'] + chainOfThoughtActivator\n",
    "        #Don't get stock on a question\n",
    "        failCounter=failCounter+1\n",
    "        if failCounter==fail_limit:\n",
    "            break\n",
    "\n",
    "    if failCounter==fail_limit:\n",
    "        continue\n",
    "\n",
    "    #find and match the option\n",
    "    next_char = llmAnswer[index+len('Answer:'):][0]\n",
    "    if next_char in truth:\n",
    "        correct=correct+1\n",
    "        print('correct')\n",
    "    else:\n",
    "        print('wrong')\n",
    "\n",
    "    #update accuracy\n",
    "    accuracy=correct/questionCounter\n",
    "    print(f\"Progress: {questionCounter/len(df_test)}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a133dd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "Beantworten Sie die folgende Frage. Schreiben Sie die Antwort am Ende Ihrer Antwort:  \n",
      "Die Frage ist:   Was besagt die Zustandsgleichung des idealen Gases?\n",
      "Plane zuerst alles Schritt für Schritt durch\n",
      "\n",
      "%%%%%%%%%%%%%\n",
      "LLM Answer:\n",
      "Die Zustandsgleichung des idealen Gases (pV = nRT) beschreibt, wie die Druck (p), Volumen (V), Temperatur (T), und die Anzahl der Teilchen (n) eines idealen Gases miteinander in Beziehung stehen. Sie zeigt, dass die Produkt der Druck und des Volumens eines idealen Gases mit der Temperatur und der Anzahl der Teilchen proportional ist. Dieser Zusammenhang ist wichtig, um die thermodynamischen Eigenschaften von\n",
      "#############################\n",
      "Beantworten Sie die folgende Frage. Schreiben Sie die Antwort am Ende Ihrer Antwort:  \n",
      "Die Frage ist:   Was ist ein ideales Gas?\n",
      "Plane zuerst alles Schritt für Schritt durch\n",
      "\n",
      "%%%%%%%%%%%%%\n",
      "LLM Answer:\n",
      "Ein ideales Gas ist ein hypothetisches Gas, das sich nach den Gesetzen des idealen Gasesverhaltens verhält. Dies bedeutet, dass die Teilchen des Gases keinerlei Kraft aufeinander ausüben und sich nicht miteinander wechselwirken. Die Energie der Teilchen wird nur durch ihre Bewegung und die Temperatur des Gases bestimmt. Dieses Modell ist ein guter Anfangspunkt für die Untersuchung der thermodynamischen Eigenschaften von Gassen.\n",
      "\n",
      "Antwort: Ein ideales\n",
      "#############################\n",
      "Beantworten Sie die folgende Frage. Schreiben Sie die Antwort am Ende Ihrer Antwort:  \n",
      "Die Frage ist:   Was besagt der Nullte Hauptsatz der Thermodynamik?\n",
      "Plane zuerst alles Schritt für Schritt durch\n",
      "\n",
      "%%%%%%%%%%%%%\n",
      "LLM Answer:\n",
      "Der Nullte Hauptsatz der Thermodynamik besagt, dass die Gesamtheit der Energie eines geschlossenen Systems konstant bleibt. Er beschreibt, dass die Energie des Systems nicht verloren geht, sondern nur zwischen verschiedenen Formen umgewandelt wird. Dieser Hauptsatz ist eine wichtige Grundlage für die Erklärung der Energieerhaltung in physikalischen Systemen.\n",
      "\n",
      "Antwort: Der Nullte Hauptsatz der Thermodynamik besagt\n",
      "#############################\n",
      "Beantworten Sie die folgende Frage. Schreiben Sie die Antwort am Ende Ihrer Antwort:  \n",
      "Die Frage ist:   Was ist der Partialdruck?\n",
      "Plane zuerst alles Schritt für Schritt durch\n",
      "\n",
      "%%%%%%%%%%%%%\n",
      "LLM Answer:\n",
      "Ein Partialdruck ist der Druck, der auf ein bestimmtes Gas in einem Gemisch auftritt. Er wird berechnet, indem der Gesamtdruck des Gemisches mit dem Anteil des betrachteten Gases multipliziert wird.\n",
      "\n",
      "Antwort: Ein Partialdruck ist der Druck, der auf ein bestimmtes Gas in einem Gemisch auftritt. Er wird berechnet, indem der Gesamtdruck des Gemisches mit dem Anteil des betrachteten Gases multipliziert wird.\n",
      "#############################\n",
      "Beantworten Sie die folgende Frage. Schreiben Sie die Antwort am Ende Ihrer Antwort:  \n",
      "Die Frage ist:   Wie lautet das Amontonssche Gesetz?\n",
      "Plane zuerst alles Schritt für Schritt durch\n",
      "\n",
      "%%%%%%%%%%%%%\n",
      "LLM Answer:\n",
      "Das Amontonssche Gesetz besagt, dass bei konstanter Temperatur die Gesamtmasse eines idealen Gases konstant bleibt, wenn die Druck- und Volumengröße verändert werden.\n",
      "\n",
      "Antwort: Das Amontonssche Gesetz besagt, dass bei konstanter Temperatur die Gesamtmasse eines idealen Gases konstant bleibt.\n"
     ]
    }
   ],
   "source": [
    "df_test_orig=pd.read_csv(test_path)\n",
    "df_test = df_test_orig.head(5)\n",
    "\n",
    "questionCounter = 0\n",
    "total_bleu_score = 0\n",
    "total_meteor_score = 0\n",
    "total_bert_scores = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "total_rouge_scores = {'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
    "                      'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
    "                      'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}}\n",
    "\n",
    "# Setting up ROUGE\n",
    "rouge = Rouge()\n",
    "\n",
    "correct=0\n",
    "promptEnding = \"[/INST]\"\n",
    "\n",
    "# this must be >= 2\n",
    "fail_limit=1\n",
    "\n",
    "# chain of thought activator, model might run out of output tokens\n",
    "USE_COT=True\n",
    "\n",
    "#this comes before the question\n",
    "testGuide='Beantworten Sie die folgende Frage. Schreiben Sie die Antwort am Ende Ihrer Antwort:  \\nDie Frage ist:   '\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    print(\"#############################\")\n",
    "    questionCounter = questionCounter + 1\n",
    "\n",
    "    #chain of thought activator\n",
    "    if USE_COT:\n",
    "        chainOfThoughtActivator='\\nPlane zuerst alles Schritt für Schritt durch\\n'\n",
    "    else:\n",
    "        chainOfThoughtActivator='\\n'\n",
    "\n",
    "    #build the prompt\n",
    "    # question=testGuide + row['Question'] + '\\na)' + row['a'] + '\\nb)' + row['b'] + '\\nc)' + row['c'] + '\\nd)' + row['d'] + chainOfThoughtActivator\n",
    "    question=testGuide + row['Question'] + chainOfThoughtActivator\n",
    "    print(question)\n",
    "\n",
    "    #true answer\n",
    "    truth=row['Answer']\n",
    "\n",
    "    #use a loop, if llm stopped before reaching the answer. ask again\n",
    "    index=-1\n",
    "    failCounter=0\n",
    "    while(index==-1):\n",
    "\n",
    "        #build the prompt\n",
    "        prompt = build_prompt(question)\n",
    "\n",
    "        #generate answer\n",
    "        result = pipe(prompt)\n",
    "        llmAnswer = result[0]['generated_text']\n",
    "        \n",
    "\n",
    "        #remove our prompt from it\n",
    "        index = llmAnswer.find(promptEnding)\n",
    "        #llmAnswer = llmAnswer[len(promptEnding)+index:]\n",
    "        llmAnswer = llmAnswer[len(promptEnding) + index:].strip()\n",
    "        print(\"%%%%%%%%%%%%%\")\n",
    "        print(\"LLM Answer:\")\n",
    "        print(llmAnswer)\n",
    "\n",
    "        #remove spaces\n",
    "        llmAnswer=llmAnswer.replace(' ','')\n",
    "\n",
    "        #find the option in response\n",
    "        index = llmAnswer.find('Answer:')\n",
    "\n",
    "        #edge case - llm stoped at the worst time\n",
    "        if(index+len('Answer:')==len(llmAnswer)):\n",
    "            index=-1\n",
    "\n",
    "        #update question for the next try. remove chain of thought\n",
    "        # question=testGuide + row['Question'] + '\\na)' + row['a'] + '\\nb)' + row['b'] + '\\nc)' + row['c'] + '\\nd)' + row['d']\n",
    "        question=testGuide + row['Question'] + chainOfThoughtActivator\n",
    "        #Don't get stock on a question\n",
    "        failCounter=failCounter+1\n",
    "        if failCounter==fail_limit:\n",
    "            break\n",
    "\n",
    "    if failCounter==fail_limit:\n",
    "        continue\n",
    "\n",
    "    reference = truth.split()\n",
    "    candidate = llmAnswer.split()\n",
    "    \n",
    "     #Compute BLEU score for the current response\n",
    "    bleu_score = sentence_bleu([reference], candidate)\n",
    "    total_bleu_score += bleu_score\n",
    "\n",
    "    # Compute ROUGE score for the current response\n",
    "    rouge_scores = rouge.get_scores(llmAnswer, truth)\n",
    "    for key in total_rouge_scores.keys():\n",
    "        total_rouge_scores[key]['f'] += rouge_scores[0][key]['f']\n",
    "        total_rouge_scores[key]['p'] += rouge_scores[0][key]['p']\n",
    "        total_rouge_scores[key]['r'] += rouge_scores[0][key]['r']\n",
    "\n",
    "    # Output results\n",
    "    print(f\"BLEU Score: {bleu_score}\")\n",
    "    print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ae79cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0\n",
      "Average ROUGE Scores: {'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# Calculate average scores\n",
    "avg_bleu_score = total_bleu_score / questionCounter\n",
    "avg_rouge_scores = {key: {k: v / questionCounter for k, v in total_rouge_scores[key].items()} for key in total_rouge_scores.keys()}\n",
    "\n",
    "print(\"Average BLEU Score:\", avg_bleu_score)\n",
    "print(\"Average ROUGE Scores:\", avg_rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c70869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428522448981\n"
     ]
    }
   ],
   "source": [
    "reference = 'this movie was awesome'\n",
    "candidate = 'this movie was awesome too'\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(candidate, reference)[0]['rouge-2']['f']\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69c80818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/serlink/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81f129fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9679878048780488\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "reference = ['this', 'movie', 'was', 'awesome']\n",
    "candidate = ['this', 'movie', 'was', 'awesome', 'too']\n",
    "score = single_meteor_score(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c938899",
   "metadata": {},
   "source": [
    "The main difference between ROUGE and BLEU is that bleu score is precision focused whereas ROUGE score focuses on recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc189a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert score: P=0.8552 R=0.8552 F1=0.8552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bert_score import score\n",
    "\n",
    "# reference and generated texts\n",
    "ref_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "gen_text = \"A fast brown fox leaps over a lazy hound.\"\n",
    "\n",
    "# compute Bert score\n",
    "P, R, F1 = score([gen_text], [ref_text], lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "\n",
    "# print results\n",
    "print(f\"Bert score: P={P.item():.4f} R={R.item():.4f} F1={F1.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b83f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert_score import BERTScorer\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from rouge import Rouge\n",
    "\n",
    "# Setup scoring systems\n",
    "bert_scorer = BERTScorer(lang=\"de\", rescale_with_baseline=True)\n",
    "rouge = Rouge()\n",
    "\n",
    "df_test_orig = pd.read_csv(test_path)\n",
    "df_test = df_test_orig.head(5)\n",
    "\n",
    "questionCounter = 0\n",
    "total_bleu_score = 0\n",
    "total_meteor_score = 0\n",
    "total_bert_scores = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "total_rouge_scores = {'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
    "                      'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
    "                      'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}}\n",
    "\n",
    "promptEnding = \"[/INST]\"\n",
    "fail_limit = 2\n",
    "USE_COT = True\n",
    "testGuide = 'Beantworten Sie die folgende Frage. Schreiben Sie die Antwort am Ende Ihrer Antwort:\\nDie Frage ist: '\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    chainOfThoughtActivator = '\\nPlane zuerst alles Schritt für Schritt durch\\n' if USE_COT else '\\n'\n",
    "    question = testGuide + row['Question'] + chainOfThoughtActivator\n",
    "    truth = row['Answer']\n",
    "    failCounter = 0\n",
    "\n",
    "    while True:\n",
    "        prompt = build_prompt(question)\n",
    "        result = pipe(prompt)\n",
    "        llmAnswer = result[0]['generated_text']\n",
    "        answer_start_index = llmAnswer.find('Answer:') + len('Answer:')\n",
    "        if answer_start_index > len('Answer:') and answer_start_index < len(llmAnswer):\n",
    "            llmAnswer = llmAnswer[answer_start_index:].strip()\n",
    "            if llmAnswer:\n",
    "                break\n",
    "        failCounter += 1\n",
    "        if failCounter >= fail_limit:\n",
    "            break\n",
    "\n",
    "    if failCounter < fail_limit:\n",
    "        # Compute METEOR Score\n",
    "        meteor_score = single_meteor_score(truth, llmAnswer)\n",
    "        total_meteor_score += meteor_score\n",
    "        \n",
    "        # Compute BERTScore\n",
    "        P, R, F1 = bert_scorer.score([llmAnswer], [truth])\n",
    "        total_bert_scores['precision'] += P.mean().item()\n",
    "        total_bert_scores['recall'] += R.mean().item()\n",
    "        total_bert_scores['f1'] += F1.mean().item()\n",
    "        \n",
    "        # Compute ROUGE Score\n",
    "        rouge_scores = rouge.get_scores(llmAnswer, truth)\n",
    "        for key in total_rouge_scores.keys():\n",
    "            total_rouge_scores[key]['f'] += rouge_scores[0][key]['f']\n",
    "            total_rouge_scores[key]['p'] += rouge_scores[0][key]['p']\n",
    "            total_rouge_scores[key]['r'] += rouge_scores[0][key]['r']\n",
    "\n",
    "        print(f\"METEOR Score: {meteor_score}\")\n",
    "        print(f\"BERT Scores: {{'Precision': {P.mean().item()}, 'Recall': {R.mean().item()}, 'F1': {F1.mean().item()}}}\")\n",
    "        print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "beb1c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop, to see average scores\n",
    "average_bert_precision = total_bert_scores['precision'] / 5\n",
    "average_bert_recall = total_bert_scores['recall'] / 5\n",
    "average_bert_f1 = total_bert_scores['f1'] / 5\n",
    "average_meteor_score = total_meteor_score / 5\n",
    "# Average ROUGE scores can also be calculated similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fc69590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_bert_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f793ede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_bert_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641048f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
