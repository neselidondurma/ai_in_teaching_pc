{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import dotenv\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv(\"test.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chatbot Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your API key from an environment variable\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instruction for our bot\n",
    "conversation = [{\"role\": \"system\", \"content\": \"You are a chemistry assistant for undergrad students.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For single question answer array\n",
    "print(\"I am your chemistry assistant. How can I help you?\")\n",
    "inputquestion = input(\"Question: \")\n",
    "\n",
    "conversation.append({\"role\": \"user\", \"content\": inputquestion})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting response from OpenAI for single question answer\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=conversation\n",
    ")\n",
    "\n",
    "assistant_response = response.choices[0].message.content\n",
    "print(f\"Assistant: {assistant_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'm here to help with chemistry questions. If you have any questions related to chemistry, feel free to ask!\n",
      "Assistant: Hello! How can I assist you today with your chemistry questions?\n",
      "Assistant: Of course! I'd be happy to help you with the periodic table. What specifically do you need assistance with? Just let me know and I'll do my best to provide you with the information you need.\n"
     ]
    }
   ],
   "source": [
    "# For multiple questions, it will remember the previous conversation\n",
    "# Question Panel will pop up, write your question there\n",
    "while True:\n",
    "    inputquestion = input(\"Question: \")\n",
    "    \n",
    "    if inputquestion.lower() == 'stop': break\n",
    "    \n",
    "    conversation.append({\"role\": \"user\",\"content\": inputquestion})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation\n",
    "    )\n",
    "    \n",
    "    assistant_response = response.choices[0].message.content\n",
    "        \n",
    "    conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "        \n",
    "    print(f\"Assistant: {assistant_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating JSON file for Q&A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        sentences = text.split('.')[:25]  # Split by '.' and take the first 25 sentences\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_in_chunks(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        sentences = text.split('.')\n",
    "        for i in range(0, len(sentences), 25):\n",
    "            yield ' '.join(sentences[i:i+25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generate_qa(text):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",  # You can specify the model here\n",
    "        prompt=f\"Generate three questions and answers based on the following text in German: {text}\",\n",
    "        max_tokens=500,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa(text):\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Generate three questions and answers based on the following text in German: {text}\"}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation,\n",
    "        max_tokens=500\n",
    "        \n",
    "    )\n",
    "    last_message = response['choices'][0]['message']['content'] if response.choices else \"No response generated.\"\n",
    "    return last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(qa_pairs, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump({\"questions\": qa_pairs}, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, output_file):\n",
    "    for text_chunk in read_sentences_in_chunks(file_path):\n",
    "        qa_text = generate_qa(text_chunk)\n",
    "        qa_pairs = [{\"question\": q.strip(), \"answer\": a.strip()} for q, a in zip(qa_text.split('\\n')[::2], qa_text.split('\\n')[1::2])]\n",
    "        save_to_json(qa_pairs, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ChatCompletion' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchapter_1.txt_extracted_text2.txt_remove_unwanted_sentences_cleaned.txt\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Adjust this to your file's name\u001b[39;00m\n\u001b[1;32m      3\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQA_dataset.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path, output_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_file\u001b[39m(file_path, output_file):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m read_sentences_in_chunks(file_path):\n\u001b[0;32m----> 3\u001b[0m         qa_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_qa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         qa_pairs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: q\u001b[38;5;241m.\u001b[39mstrip(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: a\u001b[38;5;241m.\u001b[39mstrip()} \u001b[38;5;28;01mfor\u001b[39;00m q, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(qa_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[::\u001b[38;5;241m2\u001b[39m], qa_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m])]\n\u001b[1;32m      5\u001b[0m         save_to_json(qa_pairs, output_file)\n",
      "Cell \u001b[0;32mIn[69], line 12\u001b[0m, in \u001b[0;36mgenerate_qa\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate three questions and answers based on the following text in German: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     messages\u001b[38;5;241m=\u001b[39mconversation,\n\u001b[1;32m      9\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     10\u001b[0m     \n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m last_message \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo response generated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m last_message\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ChatCompletion' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Specify the file paths and start processing\n",
    "file_path = 'chapter_1.txt_extracted_text2.txt_remove_unwanted_sentences_cleaned.txt'  # Adjust this to your file's name\n",
    "output_file = 'QA_dataset.json'\n",
    "process_file(file_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Translation to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\"Abs_1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Das ideale Gas\\nD\\nas ideale Gas stellt das einfachste physikalische System dar, anhand des- IN DIESEM ABSCHNITT\\nsen sich die Grundlagen und Aussagen der Thermodynamik anschaulich dis-\\n• Was sind Zustandsgrößen und wozu\\nkutieren lassen. Als erstes werden daher wir die sog. Zustandsgleichung des\\ndienen sie?\\nidealen Gases kennenlernen. Dieses Gasgesetz eignet sich ideal (sic), um\\nwichtige Zusammenhänge zwischen elementaren Zustandsgrößen der Ther-\\n• Was besagt die Zustandsgleichung?\\nmodynamik herzustellen und zu verstehen. Am Ende dieses Kapitels diskutie-\\nren wir die Grundlagen der kinetischen Gastheorie. Sie erlaubt die Herleitung • Der Nullte Hauptsatz der\\ndes idealen Gasgesetzes aus sehr einfachen mikroskopischen Modellvorstel- Thermodynamik\\nlungen.\\n• Was ist ein ideales Gas?\\n• Die kinetische Gastheorie\\nKAPITEL 1 15'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page11 = pdf.pages[11]\n",
    "page11.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(page):\n",
    "    \"\"\"Extract PDF text and Delete in-paragraph line breaks.\"\"\"\n",
    "    # Get text\n",
    "    extracted = page.extract_text()\n",
    "    # Delete in-paragraph line breaks\n",
    "    extracted = extracted.replace(\".\\n\", \"**/m\" # keep par breaks\n",
    "                        ).replace(\". \\n\", \"**/m\" # keep par breaks\n",
    "                        ).replace(\"\\n\", \"\" # delete in-par breaks     \n",
    "                        ).replace(\"**/m\", \".\\n\\n\") # restore par break\n",
    "    return extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Das ideale GasDas ideale Gas stellt das einfachste physikalische System dar, anhand des- IN DIESEM ABSCHNITTsen sich die Grundlagen und Aussagen der Thermodynamik anschaulich dis-• Was sind Zustandsgrößen und wozukutieren lassen. Als erstes werden daher wir die sog. Zustandsgleichung desdienen sie?idealen Gases kennenlernen. Dieses Gasgesetz eignet sich ideal (sic), umwichtige Zusammenhänge zwischen elementaren Zustandsgrößen der Ther-• Was besagt die Zustandsgleichung?modynamik herzustellen und zu verstehen. Am Ende dieses Kapitels diskutie-ren wir die Grundlagen der kinetischen Gastheorie. Sie erlaubt die Herleitung • Der Nullte Hauptsatz derdes idealen Gasgesetzes aus sehr einfachen mikroskopischen Modellvorstel- Thermodynamiklungen.\n",
      "\n",
      "• Was ist ein ideales Gas?• Die kinetische GastheorieKAPITEL 1 15\n"
     ]
    }
   ],
   "source": [
    "print(extract(page11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = GoogleTranslator(source=\"auto\", target=\"en\").translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_extracted(Extracted):\n",
    "    \"\"\"Wrapper for Google Translate with upload workaround.\"\"\"\n",
    "    # Set-up and wrap translation client\n",
    "    translate = GoogleTranslator(source='auto', target='en').translate\n",
    "    # Split input text into a list of sentences\n",
    "    sentences = sent_tokenize(Extracted)\n",
    "    # Initialize containers\n",
    "    translated_text = ''\n",
    "    source_text_chunk = ''\n",
    "    # collect chuncks of sentences, translate individually\n",
    "    for sentence in sentences:\n",
    "    # if chunck + current sentence < limit, add the sentence\n",
    "        if ((len(sentence.encode('utf-8')) +  len(source_text_chunk.encode('utf-8')) < 5000)):\n",
    "            source_text_chunk += ' ' + sentence\n",
    "        # else translate chunck and start new one with current sentence\n",
    "        else:\n",
    "            translated_text += ' ' + translate(source_text_chunk)\n",
    "            # if current sentence smaller than 5000 chars, start new chunck\n",
    "            if (len(sentence.encode('utf-8')) < 5000):\n",
    "                source_text_chunk = sentence\n",
    "            # else, replace sentence with notification message\n",
    "            else:\n",
    "                message = \"<<Omitted Word longer than 5000bytes>>\"\n",
    "                translated_text += ' ' + translate(message)\n",
    "                # Re-set text container to empty\n",
    "                source_text_chunk = ''\n",
    "  # Translate the final chunk of input text, if there is any valid   text left to translate\n",
    "    if translate(source_text_chunk) != None:\n",
    "        translated_text += ' ' + translate(source_text_chunk)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/serlink/nltk_data'\n    - '/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/nltk_data'\n    - '/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/share/nltk_data'\n    - '/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m extracted \u001b[38;5;241m=\u001b[39m extract(pdf\u001b[38;5;241m.\u001b[39mpages[\u001b[38;5;241m12\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_extracted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m500\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(translated)\n",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m, in \u001b[0;36mtranslate_extracted\u001b[0;34m(Extracted)\u001b[0m\n\u001b[1;32m      4\u001b[0m translate \u001b[38;5;241m=\u001b[39m GoogleTranslator(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtranslate\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Split input text into a list of sentences\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mExtracted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize containers\u001b[39;00m\n\u001b[1;32m      8\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/serlink/nltk_data'\n    - '/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/nltk_data'\n    - '/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/share/nltk_data'\n    - '/opt/homebrew/Caskroom/mambaforge/base/envs/hertel_pc/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "extracted = extract(pdf.pages[12])\n",
    "translated = translate_extracted(extracted)[:500]\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hertel_pc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
